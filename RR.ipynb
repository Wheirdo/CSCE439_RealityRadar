{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FvBLMgFD6Dhw"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sgull\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#For SVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#Used to parse csv file\n",
    "import csv\n",
    "\n",
    "#Used for regex (removing dates, usernames, etc)\n",
    "import re\n",
    "#Used for removing stop words\n",
    "import nltk.corpus\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Used for stemming\n",
    "from nltk.stem import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_data(dataset_location):\n",
    "  #Regex patterns\n",
    "  date_pattern = r'\\b(?:january|february|march|april|may|june|july|august|september|october|november|december)\\s+\\d{1,2}\\s+\\d{4}\\b'\n",
    "  number_space_pattern = re.compile(r'(\\d)([^\\d\\s])')\n",
    "  punctuation_space_pattern = re.compile(r'([^\\w\\s])(?!\\s)')\n",
    "  username_pattern = r'\\(@\\w+\\)'\n",
    "\n",
    "  #nltk\n",
    "  port_stemmer = PorterStemmer()\n",
    "  stop_words = stopwords.words('english')\n",
    "\n",
    "  output = []\n",
    "  with open(dataset_location, 'r', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    #skip first line\n",
    "    next(csv_reader)\n",
    "    \n",
    "    for row in csv_reader:\n",
    "      line = row[0] + \" \" + row[1]\n",
    "      \n",
    "      #lowercase\n",
    "      line = line.lower()\n",
    "      \n",
    "      #Remove the \"photo by Getty Images\" stuff\n",
    "      index = line.rfind('photo by')\n",
    "      if (index != -1):\n",
    "        line = line[:index]\n",
    "        \n",
    "      #U.S gets turned into 'u s' which then gets filtered by the stop words\n",
    "      line = line.replace(\"u.s.\",\"usa\")\n",
    "      \n",
    "      #Remove usernames (@Username)\n",
    "      line = re.sub(username_pattern, '', line)\n",
    "        \n",
    "      #Remove double spaces\n",
    "      line = line.replace(\"  \", \" \")\n",
    "        \n",
    "      #Add spaces when numbers are right next to text\n",
    "      line = re.sub(number_space_pattern, r'\\1 \\2', line)\n",
    "      \n",
    "      #Add spaces when puncation is right next to text\n",
    "      line = re.sub(punctuation_space_pattern, r'\\1 ', line)\n",
    "          \n",
    "      #Remove special characters and links\n",
    "      line = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t!])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", line)\n",
    "      \n",
    "      #Remove dates\n",
    "      line = re.sub(date_pattern, '', line)\n",
    "\n",
    "      #Remove stop words\n",
    "      line = \" \".join([word for word in line.split(\" \") if word not in (stop_words)])\n",
    "      \n",
    "      #Remove double spaces (again)\n",
    "      line = line.replace(\"  \", \" \")\n",
    "      \n",
    "      \n",
    "      line_words = line.split(\" \")\n",
    "      stem_output = \"\"\n",
    "      for word in line_words:\n",
    "        word = port_stemmer.stem(word)\n",
    "        \n",
    "        stem_output += word + \" \"\n",
    "        \n",
    "      #Remove space at the very end\n",
    "      line = stem_output[:-1]  \n",
    "      \n",
    "      \n",
    "      #Remove double spaces (last time, for good measure)\n",
    "      line = line.replace(\"  \", \" \")\n",
    "        \n",
    "      output.append(line)\n",
    "      \n",
    "  csv_file.close()\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_data = clean_data(\"Fake.csv\")\n",
    "true_data = clean_data(\"True.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"fake_clean.txt\", 'w') as file:\n",
    "    for item in fake_data:\n",
    "        file.write(str(item) + '\\n')\n",
    "\n",
    "file.close()       \n",
    "        \n",
    "with open(\"true_clean.txt\", 'w') as file:\n",
    "    for item in true_data:\n",
    "        file.write(str(item) + '\\n')\n",
    "        \n",
    "file.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_fake_data = fake_data[:20000]\n",
    "svm_true_data = true_data[:20000]\n",
    "\n",
    "#Creating lables\n",
    "labels = [1] * len(svm_fake_data) + [0] * len(svm_true_data)\n",
    "\n",
    "#splitting data into training and testing sets\n",
    "all_articles = svm_fake_data + svm_true_data\n",
    "training_articles, testing_articles, training_labels, testing_labels = train_test_split(all_articles, labels, random_state=42, test_size = 0.2)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(input='content', stop_words='english', decode_error = \"ignore\")\n",
    "tfidf_vector = tfidf_vectorizer.fit_transform(training_articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10000, kernel='linear')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM = SVC(kernel='linear', C=10000)\n",
    "\n",
    "SVM.fit(tfidf_vector, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3960   26]\n",
      " [  32 3982]]\n",
      "true positives: 3982\n",
      "false negatives: 32\n",
      "true negatives: 3960\n",
      "false positives: 26\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9920    0.9935    0.9927      3986\n",
      "           1     0.9935    0.9920    0.9928      4014\n",
      "\n",
      "    accuracy                         0.9928      8000\n",
      "   macro avg     0.9927    0.9928    0.9927      8000\n",
      "weighted avg     0.9928    0.9928    0.9928      8000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tdidf_vector_TA = tfidf_vectorizer.transform(testing_articles)\n",
    "\n",
    "predictions = SVM.predict(tdidf_vector_TA)\n",
    "\n",
    "\n",
    "confused = confusion_matrix(testing_labels,predictions)\n",
    "print(confused)\n",
    "print(\"true positives: \" + str(confused[1][1]))\n",
    "print(\"false negatives: \" + str(confused[1][0]))\n",
    "print(\"true negatives: \"  +str(confused[0][0]))\n",
    "print(\"false positives: \" + str(confused[0][1]))\n",
    "\n",
    "class_report_lin = classification_report(testing_labels, predictions, digits = 4)\n",
    "print()\n",
    "print(class_report_lin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qD9asa6D5I5Y"
   },
   "source": [
    "Tfidf analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating lables\n",
    "#A label of 1 is fake news, a label of 0 is real news\n",
    "\n",
    "labels = [1] * len(fake_data) + [0] * len(true_data)\n",
    "\n",
    "#splitting data into training and testing sets\n",
    "all_articles = fake_data + true_data\n",
    "training_articles, testing_articles, training_labels, testing_labels = train_test_split(all_articles, labels, random_state=42, test_size = 0.2)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(input='content', stop_words='english', decode_error = \"ignore\")\n",
    "tfidf_vector = tfidf_vectorizer.fit_transform(training_articles)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classify With RFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4025  222]\n",
      " [ 145 4588]]\n",
      "true positives: 4588\n",
      "false negatives: 145\n",
      "true negatives: 4025\n",
      "false positives: 222\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9652    0.9477    0.9564      4247\n",
      "           1     0.9538    0.9694    0.9615      4733\n",
      "\n",
      "    accuracy                         0.9591      8980\n",
      "   macro avg     0.9595    0.9585    0.9590      8980\n",
      "weighted avg     0.9592    0.9591    0.9591      8980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "one_word_model = RandomForestClassifier(n_estimators = 50, max_depth = 10)\n",
    "one_word_model.fit(tfidf_vector, training_labels)\n",
    "tfidf_vector = tfidf_vectorizer.transform(testing_articles)\n",
    "\n",
    "predictions = one_word_model.predict(tfidf_vector)\n",
    "\n",
    "confused = confusion_matrix(testing_labels, predictions)\n",
    "print(confused)\n",
    "print(\"true positives: \" + str(confused[1][1]))\n",
    "print(\"false negatives: \" + str(confused[1][0]))\n",
    "print(\"true negatives: \"  +str(confused[0][0]))\n",
    "print(\"false positives: \" + str(confused[0][1]))\n",
    "\n",
    "class_report_lin = classification_report(testing_labels, predictions, digits = 4)\n",
    "print()\n",
    "print(class_report_lin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n-gram approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(input='content', ngram_range=(2, 2), stop_words='english', decode_error = \"ignore\")\n",
    "count_vector = count_vectorizer.fit_transform(training_articles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print out some features to check that correctly sized n-grams were produced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df= pd.DataFrame(count_vector[0].T.todense(), index=count_vectorizer.get_feature_names_out(), columns = [\"Frequency\"])\n",
    "first_25 = count_df.sort_values(by=[\"Frequency\"], ascending=[False]).head(25)\n",
    "\n",
    "print(first_25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classify with RFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2145 2102]\n",
      " [  32 4701]]\n",
      "true positives: 4701\n",
      "false negatives: 32\n",
      "true negatives: 2145\n",
      "false positives: 2102\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9853    0.5051    0.6678      4247\n",
      "           1     0.6910    0.9932    0.8150      4733\n",
      "\n",
      "    accuracy                         0.7624      8980\n",
      "   macro avg     0.8382    0.7492    0.7414      8980\n",
      "weighted avg     0.8302    0.7624    0.7454      8980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "multi_word_model = RandomForestClassifier(n_estimators = 50, max_depth = 10)\n",
    "multi_word_model.fit(count_vector, training_labels)\n",
    "count_vector = count_vectorizer.transform(testing_articles)\n",
    "\n",
    "predictions = multi_word_model.predict(count_vector)\n",
    "\n",
    "confused = confusion_matrix(testing_labels, predictions)\n",
    "print(confused)\n",
    "print(\"true positives: \" + str(confused[1][1]))\n",
    "print(\"false negatives: \" + str(confused[1][0]))\n",
    "print(\"true negatives: \"  +str(confused[0][0]))\n",
    "print(\"false positives: \" + str(confused[0][1]))\n",
    "\n",
    "class_report_lin = classification_report(testing_labels, predictions, digits = 4)\n",
    "print()\n",
    "print(class_report_lin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
