{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "FvBLMgFD6Dhw"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aswhe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#For SVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#Used to parse csv file\n",
    "import csv\n",
    "\n",
    "#Used for regex (removing dates, usernames, etc)\n",
    "import re\n",
    "#Used for removing stop words\n",
    "import nltk.corpus\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Used for stemming\n",
    "from nltk.stem import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_data(dataset_location):\n",
    "  #Regex patterns\n",
    "  date_pattern = r'\\b(?:january|february|march|april|may|june|july|august|september|october|november|december)\\s+\\d{1,2}\\s+\\d{4}\\b'\n",
    "  number_space_pattern = re.compile(r'(\\d)([^\\d\\s])')\n",
    "  punctuation_space_pattern = re.compile(r'([^\\w\\s])(?!\\s)')\n",
    "  username_pattern = r'\\(@\\w+\\)'\n",
    "\n",
    "  #nltk\n",
    "  port_stemmer = PorterStemmer()\n",
    "  stop_words = stopwords.words('english')\n",
    "\n",
    "  output = []\n",
    "  with open(dataset_location, 'r', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    #skip first line\n",
    "    next(csv_reader)\n",
    "    \n",
    "    for row in csv_reader:\n",
    "      line = row[0] + \" \" + row[1]\n",
    "      \n",
    "      #lowercase\n",
    "      line = line.lower()\n",
    "      \n",
    "      #Remove the \"photo by Getty Images\" stuff\n",
    "      index = line.rfind('photo by')\n",
    "      if (index != -1):\n",
    "        line = line[:index]\n",
    "        \n",
    "      #U.S gets turned into 'u s' which then gets filtered by the stop words\n",
    "      line = line.replace(\"u.s.\",\"usa\")\n",
    "      \n",
    "      #Remove usernames (@Username)\n",
    "      line = re.sub(username_pattern, '', line)\n",
    "        \n",
    "      #Remove double spaces\n",
    "      line = line.replace(\"  \", \" \")\n",
    "        \n",
    "      #Add spaces when numbers are right next to text\n",
    "      line = re.sub(number_space_pattern, r'\\1 \\2', line)\n",
    "      \n",
    "      #Add spaces when puncation is right next to text\n",
    "      line = re.sub(punctuation_space_pattern, r'\\1 ', line)\n",
    "          \n",
    "      #Remove special characters and links\n",
    "      line = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t!])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", line)\n",
    "      \n",
    "      #Remove dates\n",
    "      line = re.sub(date_pattern, '', line)\n",
    "\n",
    "      #Remove stop words\n",
    "      line = \" \".join([word for word in line.split(\" \") if word not in (stop_words)])\n",
    "      \n",
    "      #Remove double spaces (again)\n",
    "      line = line.replace(\"  \", \" \")\n",
    "      \n",
    "      '''\n",
    "      #Stemming\n",
    "      line_words = line.split(\" \")\n",
    "      stem_output = \"\"\n",
    "      for word in line_words:\n",
    "        word = port_stemmer.stem(word)\n",
    "        \n",
    "        stem_output += word + \" \"\n",
    "        \n",
    "      #Remove space at the very end\n",
    "      line = stem_output[:-1]  \n",
    "      '''\n",
    "      \n",
    "      #Remove double spaces (last time, for good measure)\n",
    "      line = line.replace(\"  \", \" \")\n",
    "        \n",
    "      output.append(line)\n",
    "      \n",
    "  csv_file.close()\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_data = clean_data(\"Fake.csv\")\n",
    "true_data = clean_data(\"True.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"fake_clean.txt\", 'w') as file:\n",
    "    for item in fake_data:\n",
    "        file.write(str(item) + '\\n')\n",
    "\n",
    "file.close()       \n",
    "        \n",
    "with open(\"true_clean.txt\", 'w') as file:\n",
    "    for item in true_data:\n",
    "        file.write(str(item) + '\\n')\n",
    "        \n",
    "file.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_fake_data = fake_data[:20000]\n",
    "svm_true_data = true_data[:20000]\n",
    "\n",
    "#Creating lables\n",
    "labels = [1] * len(svm_fake_data) + [0] * len(svm_true_data)\n",
    "\n",
    "#splitting data into training and testing sets\n",
    "all_articles = svm_fake_data + svm_true_data\n",
    "training_articles, testing_articles, training_labels, testing_labels = train_test_split(all_articles, labels, random_state=42, test_size = 0.2)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(input='content', stop_words='english', decode_error = \"ignore\")\n",
    "tfidf_vector = tfidf_vectorizer.fit_transform(training_articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=100, kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=100, kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=100, kernel='linear')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM = SVC(kernel='linear', C=10000)\n",
    "\n",
    "SVM.fit(tfidf_vector, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3964   22]\n",
      " [  32 3982]]\n",
      "true positives: 3982\n",
      "false negatives: 32\n",
      "true negatives: 3964\n",
      "false positives: 22\n"
     ]
    }
   ],
   "source": [
    "tdidf_vector_TA = tfidf_vectorizer.transform(testing_articles)\n",
    "\n",
    "predictions = SVM.predict(tdidf_vector_TA)\n",
    "\n",
    "\n",
    "confused = confusion_matrix(testing_labels,predictions)\n",
    "print(confused)\n",
    "print(\"true positives: \" + str(confused[1][1]))\n",
    "print(\"false negatives: \" + str(confused[1][0]))\n",
    "print(\"true negatives: \"  +str(confused[0][0]))\n",
    "print(\"false positives: \" + str(confused[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qD9asa6D5I5Y"
   },
   "source": [
    "Do tfidf analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating lables\n",
    "#A label of 1 is fake news, a label of 0 is real news\n",
    "\n",
    "labels = [1] * len(fake_data) + [0] * len(true_data)\n",
    "\n",
    "#splitting data into training and testing sets\n",
    "all_articles = fake_data + true_data\n",
    "training_articles, testing_articles, training_labels, testing_labels = train_test_split(all_articles, labels, random_state=42, test_size = 0.2)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(input='content', stop_words='english', decode_error = \"ignore\")\n",
    "tfidf_vector = tfidf_vectorizer.fit_transform(training_articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 59960 features, but SVC is expecting 108487 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\aswhe\\Downloads\\CSCE439\\Final_Project\\RR.ipynb Cell 8\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/aswhe/Downloads/CSCE439/Final_Project/RR.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tdidf_vector_TestArt \u001b[39m=\u001b[39m tfidf_vectorizer\u001b[39m.\u001b[39mfit_transform(testing_articles)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/aswhe/Downloads/CSCE439/Final_Project/RR.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m predictions \u001b[39m=\u001b[39m SVM\u001b[39m.\u001b[39mpredict(tdidf_vector_TestArt)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/aswhe/Downloads/CSCE439/Final_Project/RR.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m confused \u001b[39m=\u001b[39m confusion_matrix(testing_labels,predictions)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/aswhe/Downloads/CSCE439/Final_Project/RR.ipynb#X22sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(confused)\n",
      "File \u001b[1;32mc:\\Users\\aswhe\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:818\u001b[0m, in \u001b[0;36mBaseSVC.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    816\u001b[0m     y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecision_function(X), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    817\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 818\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mpredict(X)\n\u001b[0;32m    819\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_\u001b[39m.\u001b[39mtake(np\u001b[39m.\u001b[39masarray(y, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mintp))\n",
      "File \u001b[1;32mc:\\Users\\aswhe\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:431\u001b[0m, in \u001b[0;36mBaseLibSVM.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m    416\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Perform regression on samples in X.\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \n\u001b[0;32m    418\u001b[0m \u001b[39m    For an one-class model, +1 (inlier) or -1 (outlier) is returned.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[39m        The predicted values.\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 431\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_for_predict(X)\n\u001b[0;32m    432\u001b[0m     predict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sparse_predict \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sparse \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dense_predict\n\u001b[0;32m    433\u001b[0m     \u001b[39mreturn\u001b[39;00m predict(X)\n",
      "File \u001b[1;32mc:\\Users\\aswhe\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:611\u001b[0m, in \u001b[0;36mBaseLibSVM._validate_for_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    608\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m    610\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mcallable\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel):\n\u001b[1;32m--> 611\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_data(\n\u001b[0;32m    612\u001b[0m         X,\n\u001b[0;32m    613\u001b[0m         accept_sparse\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcsr\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    614\u001b[0m         dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat64,\n\u001b[0;32m    615\u001b[0m         order\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mC\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    616\u001b[0m         accept_large_sparse\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    617\u001b[0m         reset\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    618\u001b[0m     )\n\u001b[0;32m    620\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sparse \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m sp\u001b[39m.\u001b[39misspmatrix(X):\n\u001b[0;32m    621\u001b[0m     X \u001b[39m=\u001b[39m sp\u001b[39m.\u001b[39mcsr_matrix(X)\n",
      "File \u001b[1;32mc:\\Users\\aswhe\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:625\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    622\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    624\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m--> 625\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_n_features(X, reset\u001b[39m=\u001b[39mreset)\n\u001b[0;32m    627\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\aswhe\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:414\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    411\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    413\u001b[0m \u001b[39mif\u001b[39;00m n_features \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 414\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    415\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX has \u001b[39m\u001b[39m{\u001b[39;00mn_features\u001b[39m}\u001b[39;00m\u001b[39m features, but \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    416\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mis expecting \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_\u001b[39m}\u001b[39;00m\u001b[39m features as input.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    417\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 59960 features, but SVC is expecting 108487 features as input."
     ]
    }
   ],
   "source": [
    "tdidf_vector_TestArt = tfidf_vectorizer.fit_transform(testing_articles)\n",
    "\n",
    "predictions = SVM.predict(tdidf_vector_TestArt)\n",
    "\n",
    "\n",
    "confused = confusion_matrix(testing_labels,predictions)\n",
    "print(confused)\n",
    "print(\"true positives: \" + str(confused[1][1]))\n",
    "print(\"false negatives: \" + str(confused[1][0]))\n",
    "print(\"true negatives: \"  +str(confused[0][0]))\n",
    "print(\"false positives: \" + str(confused[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classify With RFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4100  147]\n",
      " [  84 4649]]\n",
      "true positives: 4649\n",
      "false negatives: 84\n",
      "true negatives: 4100\n",
      "false positives: 147\n"
     ]
    }
   ],
   "source": [
    "one_word_model = RandomForestClassifier(n_estimators = 50, max_depth = 10)\n",
    "one_word_model.fit(tfidf_vector, training_labels)\n",
    "tfidf_vector = tfidf_vectorizer.transform(testing_articles)\n",
    "\n",
    "predictions = one_word_model.predict(tfidf_vector)\n",
    "\n",
    "confused = confusion_matrix(testing_labels, predictions)\n",
    "print(confused)\n",
    "print(\"true positives: \" + str(confused[1][1]))\n",
    "print(\"false negatives: \" + str(confused[1][0]))\n",
    "print(\"true negatives: \"  +str(confused[0][0]))\n",
    "print(\"false positives: \" + str(confused[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\aswhe\\Downloads\\CSCE439\\Final_Project\\RR.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/aswhe/Downloads/CSCE439/Final_Project/RR.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m count_vectorizer \u001b[39m=\u001b[39m CountVectorizer(\u001b[39minput\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m, ngram_range\u001b[39m=\u001b[39m(\u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m), stop_words\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m, decode_error \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/aswhe/Downloads/CSCE439/Final_Project/RR.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m count_vector \u001b[39m=\u001b[39m count_vectorizer\u001b[39m.\u001b[39mfit_transform(training_articles)\n",
      "File \u001b[1;32mc:\\Users\\aswhe\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\aswhe\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1383\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1375\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1376\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1377\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1378\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1379\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1380\u001b[0m             )\n\u001b[0;32m   1381\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1383\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_count_vocab(raw_documents, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfixed_vocabulary_)\n\u001b[0;32m   1385\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1386\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\aswhe\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1270\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1268\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[0;32m   1269\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[1;32m-> 1270\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1271\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1272\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32mc:\\Users\\aswhe\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:115\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mif\u001b[39;00m ngrams \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    114\u001b[0m     \u001b[39mif\u001b[39;00m stop_words \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 115\u001b[0m         doc \u001b[39m=\u001b[39m ngrams(doc, stop_words)\n\u001b[0;32m    116\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    117\u001b[0m         doc \u001b[39m=\u001b[39m ngrams(doc)\n",
      "File \u001b[1;32mc:\\Users\\aswhe\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:244\u001b[0m, in \u001b[0;36m_VectorizerMixin._word_ngrams\u001b[1;34m(self, tokens, stop_words)\u001b[0m\n\u001b[0;32m    238\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    239\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mnp.nan is an invalid document, expected byte or unicode string.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    240\u001b[0m         )\n\u001b[0;32m    242\u001b[0m     \u001b[39mreturn\u001b[39;00m doc\n\u001b[1;32m--> 244\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_word_ngrams\u001b[39m(\u001b[39mself\u001b[39m, tokens, stop_words\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    245\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Turn tokens into a sequence of n-grams after stop words filtering\"\"\"\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[39m# handle stop words\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(input='content', ngram_range=(2, 2), stop_words='english', decode_error = \"ignore\")\n",
    "count_vector = count_vectorizer.fit_transform(training_articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Frequency\n",
      "said greek                       1\n",
      "usa president                    1\n",
      "decision recognize               1\n",
      "tramples law                     1\n",
      "usa jerusalem                    1\n",
      "island cyprus                    1\n",
      "recognize jerusalem              1\n",
      "athens talks                     1\n",
      "laws erdogan                     1\n",
      "turkey erdogan                   1\n",
      "reuters turkish                  1\n",
      "avoiding talks                   1\n",
      "wanted lasting                   1\n",
      "donald trump                     1\n",
      "law athens                       1\n",
      "lasting solution                 1\n",
      "jerusalem decision               1\n",
      "israel trampling                 1\n",
      "tsipras said                     1\n",
      "prime minister                   1\n",
      "erdogan says                     1\n",
      "trampling international          1\n",
      "trump unfortunate                1\n",
      "says usa                         1\n",
      "turkey wanted                    1\n"
     ]
    }
   ],
   "source": [
    "count_df= pd.DataFrame(count_vector[0].T.todense(), index=count_vectorizer.get_feature_names_out(), columns = [\"Frequency\"])\n",
    "first_25 = count_df.sort_values(by=[\"Frequency\"], ascending=[False]).head(25)\n",
    "\n",
    "print(first_25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1846 2401]\n",
      " [  23 4710]]\n",
      "true positives: 4710\n",
      "false negatives: 23\n",
      "true negatives: 1846\n",
      "false positives: 2401\n"
     ]
    }
   ],
   "source": [
    "two_word_model = RandomForestClassifier(n_estimators = 50, max_depth = 10)\n",
    "two_word_model.fit(count_vector, training_labels)\n",
    "count_vector = count_vectorizer.transform(testing_articles)\n",
    "\n",
    "predictions = two_word_model.predict(count_vector)\n",
    "\n",
    "confused = confusion_matrix(testing_labels, predictions)\n",
    "print(confused)\n",
    "print(\"true positives: \" + str(confused[1][1]))\n",
    "print(\"false negatives: \" + str(confused[1][0]))\n",
    "print(\"true negatives: \"  +str(confused[0][0]))\n",
    "print(\"false positives: \" + str(confused[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
