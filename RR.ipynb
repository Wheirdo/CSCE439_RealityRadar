{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "FvBLMgFD6Dhw"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\aswhe\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "#Used to parse csv file\n",
        "import csv\n",
        "\n",
        "#Used for regex (removing dates, usernames, etc)\n",
        "import re\n",
        "#Used for removing stop words\n",
        "import nltk.corpus\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#Used for stemming\n",
        "from nltk.stem import *\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def clean_data(dataset_location):\n",
        "  #Regex patterns\n",
        "  date_pattern = r'\\b(?:january|february|march|april|may|june|july|august|september|october|november|december)\\s+\\d{1,2}\\s+\\d{4}\\b'\n",
        "  number_space_pattern = re.compile(r'(\\d)([^\\d\\s])')\n",
        "  punctuation_space_pattern = re.compile(r'([^\\w\\s])(?!\\s)')\n",
        "  username_pattern = r'\\(@\\w+\\)'\n",
        "\n",
        "  #nltk\n",
        "  port_stemmer = PorterStemmer()\n",
        "  stop_words = stopwords.words('english')\n",
        "\n",
        "  output = []\n",
        "  with open(dataset_location, 'r', encoding='utf-8') as csv_file:\n",
        "    csv_reader = csv.reader(csv_file)\n",
        "    #skip first line\n",
        "    next(csv_reader)\n",
        "    \n",
        "    for row in csv_reader:\n",
        "      line = row[0] + \" \" + row[1]\n",
        "      \n",
        "      #lowercase\n",
        "      line = line.lower()\n",
        "      \n",
        "      #Remove the \"photo by Getty Images\" stuff\n",
        "      index = line.rfind('photo by')\n",
        "      if (index != -1):\n",
        "        line = line[:index]\n",
        "        \n",
        "      #U.S gets turned into 'u s' which then gets filtered by the stop words\n",
        "      line = line.replace(\"u.s.\",\"usa\")\n",
        "      \n",
        "          \n",
        "          \n",
        "      #Remove usernames (@Username)\n",
        "      line = re.sub(username_pattern, '', line)\n",
        "        \n",
        "      #Remove double spaces\n",
        "      line = line.replace(\"  \", \" \")\n",
        "        \n",
        "      #Add spaces when numbers are right next to text\n",
        "      line = re.sub(number_space_pattern, r'\\1 \\2', line)\n",
        "      \n",
        "      #Add spaces when puncation is right next to text\n",
        "      line = re.sub(punctuation_space_pattern, r'\\1 ', line)\n",
        "          \n",
        "      #Remove special characters and links\n",
        "      line = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t!'])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", line)\n",
        "      \n",
        "      #Remove dates\n",
        "      line = re.sub(date_pattern, '', line)\n",
        "\n",
        "      #Remove stop words\n",
        "      line = \" \".join([word for word in line.split(\" \") if word not in (stop_words)])\n",
        "      \n",
        "      #Remove double spaces (again)\n",
        "      line = line.replace(\"  \", \" \")\n",
        "      \n",
        "      #Stemming\n",
        "      line_words = line.split(\" \")\n",
        "      stem_output = \"\"\n",
        "      for word in line_words:\n",
        "        word = port_stemmer.stem(word)\n",
        "        \n",
        "        stem_output += word + \" \"\n",
        "        \n",
        "      #Remove space at the very end\n",
        "      line = stem_output[:-1]  \n",
        "      \n",
        "      #Remove double spaces (last time, for good measure)\n",
        "      line = line.replace(\"  \", \" \")\n",
        "        \n",
        "      output.append(line)\n",
        "      \n",
        "  csv_file.close()\n",
        "  return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "fake_data = clean_data(\"Fake.csv\")\n",
        "true_data = clean_data(\"True.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "with open(\"fake_clean.txt\", 'w') as file:\n",
        "    for item in fake_data:\n",
        "        file.write(str(item) + '\\n')\n",
        "\n",
        "file.close()       \n",
        "        \n",
        "with open(\"true_clean.txt\", 'w') as file:\n",
        "    for item in true_data:\n",
        "        file.write(str(item) + '\\n')\n",
        "        \n",
        "file.close() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qD9asa6D5I5Y"
      },
      "source": [
        "Do tfidf analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "empty vocabulary; perhaps the documents only contain stop words",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\aswhe\\Downloads\\CSCE439\\Final_Project\\RR.ipynb Cell 6\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/aswhe/Downloads/CSCE439/Final_Project/RR.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m testing_labels \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/aswhe/Downloads/CSCE439/Final_Project/RR.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m tfidf_vectorizer \u001b[39m=\u001b[39m TfidfVectorizer(\u001b[39minput\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m, stop_words\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m, decode_error \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/aswhe/Downloads/CSCE439/Final_Project/RR.ipynb#W4sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m tfidf_vector \u001b[39m=\u001b[39m tfidf_vectorizer\u001b[39m.\u001b[39mfit_transform(training_articles)\n",
            "File \u001b[1;32mc:\\Users\\aswhe\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2126\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2119\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params()\n\u001b[0;32m   2120\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf \u001b[39m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2121\u001b[0m     norm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm,\n\u001b[0;32m   2122\u001b[0m     use_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_idf,\n\u001b[0;32m   2123\u001b[0m     smooth_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmooth_idf,\n\u001b[0;32m   2124\u001b[0m     sublinear_tf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublinear_tf,\n\u001b[0;32m   2125\u001b[0m )\n\u001b[1;32m-> 2126\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mfit_transform(raw_documents)\n\u001b[0;32m   2127\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mfit(X)\n\u001b[0;32m   2128\u001b[0m \u001b[39m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2129\u001b[0m \u001b[39m# we set copy to False\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\aswhe\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\aswhe\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1383\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1375\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1376\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1377\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1378\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1379\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1380\u001b[0m             )\n\u001b[0;32m   1381\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1383\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_count_vocab(raw_documents, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfixed_vocabulary_)\n\u001b[0;32m   1385\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1386\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\aswhe\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1289\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1287\u001b[0m     vocabulary \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1288\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1289\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1290\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1291\u001b[0m         )\n\u001b[0;32m   1293\u001b[0m \u001b[39mif\u001b[39;00m indptr[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m np\u001b[39m.\u001b[39miinfo(np\u001b[39m.\u001b[39mint32)\u001b[39m.\u001b[39mmax:  \u001b[39m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1294\u001b[0m     \u001b[39mif\u001b[39;00m _IS_32BIT:\n",
            "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
          ]
        }
      ],
      "source": [
        "#Things that need to be defined\n",
        "training_articles = []\n",
        "testing_articles = []\n",
        "training_labels = []\n",
        "testing_labels = []\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(input='content', stop_words='english', decode_error = \"ignore\")\n",
        "tfidf_vector = tfidf_vectorizer.fit_transform(training_articles)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Classify With RFC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = RandomForestClassifier(n_estimators = 10, max_depth = 10)\n",
        "model.fit(tfidf_vector, training_labels)\n",
        "tfidf_vector = tfidf_vectorizer.transform(testing_articles)\n",
        "\n",
        "predictions = model.predict(tfidf_vector)\n",
        "\n",
        "confused = confusion_matrix(testing_labels, predictions)\n",
        "print(confused)\n",
        "print(\"true positives: \" + str(confused[1][1]))\n",
        "print(\"false negatives: \" + str(confused[1][0]))\n",
        "print(\"true negatives: \"  +str(confused[0][0]))\n",
        "print(\"false positives: \" + str(confused[0][1]))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
