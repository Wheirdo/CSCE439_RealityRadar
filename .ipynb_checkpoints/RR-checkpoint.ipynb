{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "FvBLMgFD6Dhw"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sgull\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Used to parse csv file\n",
    "import csv\n",
    "\n",
    "#Used for regex (removing dates, usernames, etc)\n",
    "import re\n",
    "#Used for removing stop words\n",
    "import nltk.corpus\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Used for stemming\n",
    "from nltk.stem import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_data(dataset_location):\n",
    "  #Regex patterns\n",
    "  date_pattern = r'\\b(?:january|february|march|april|may|june|july|august|september|october|november|december)\\s+\\d{1,2}\\s+\\d{4}\\b'\n",
    "  number_space_pattern = re.compile(r'(\\d)([^\\d\\s])')\n",
    "  punctuation_space_pattern = re.compile(r'([^\\w\\s])(?!\\s)')\n",
    "  username_pattern = r'\\(@\\w+\\)'\n",
    "\n",
    "  #nltk\n",
    "  port_stemmer = PorterStemmer()\n",
    "  stop_words = stopwords.words('english')\n",
    "\n",
    "  output = []\n",
    "  with open(dataset_location, 'r', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    #skip first line\n",
    "    next(csv_reader)\n",
    "    \n",
    "    for row in csv_reader:\n",
    "      line = row[0] + \" \" + row[1]\n",
    "      \n",
    "      #lowercase\n",
    "      line = line.lower()\n",
    "      \n",
    "      #Remove the \"photo by Getty Images\" stuff\n",
    "      index = line.rfind('photo by')\n",
    "      if (index != -1):\n",
    "        line = line[:index]\n",
    "        \n",
    "      #U.S gets turned into 'u s' which then gets filtered by the stop words\n",
    "      line = line.replace(\"u.s.\",\"usa\")\n",
    "      \n",
    "          \n",
    "          \n",
    "      #Remove usernames (@Username)\n",
    "      line = re.sub(username_pattern, '', line)\n",
    "        \n",
    "      #Remove double spaces\n",
    "      line = line.replace(\"  \", \" \")\n",
    "        \n",
    "      #Add spaces when numbers are right next to text\n",
    "      line = re.sub(number_space_pattern, r'\\1 \\2', line)\n",
    "      \n",
    "      #Add spaces when puncation is right next to text\n",
    "      line = re.sub(punctuation_space_pattern, r'\\1 ', line)\n",
    "          \n",
    "      #Remove special characters and links\n",
    "      line = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t!'])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", line)\n",
    "      \n",
    "      #Remove dates\n",
    "      line = re.sub(date_pattern, '', line)\n",
    "\n",
    "      #Remove stop words\n",
    "      line = \" \".join([word for word in line.split(\" \") if word not in (stop_words)])\n",
    "      \n",
    "      #Remove double spaces (again)\n",
    "      line = line.replace(\"  \", \" \")\n",
    "      \n",
    "      #Stemming\n",
    "      line_words = line.split(\" \")\n",
    "      stem_output = \"\"\n",
    "      for word in line_words:\n",
    "        word = port_stemmer.stem(word)\n",
    "        \n",
    "        stem_output += word + \" \"\n",
    "        \n",
    "      #Remove space at the very end\n",
    "      line = stem_output[:-1]  \n",
    "      \n",
    "      #Remove double spaces (last time, for good measure)\n",
    "      line = line.replace(\"  \", \" \")\n",
    "        \n",
    "      output.append(line)\n",
    "      \n",
    "  csv_file.close()\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_data = clean_data(\"Fake.csv\")\n",
    "true_data = clean_data(\"True.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"fake_clean.txt\", 'w') as file:\n",
    "    for item in fake_data:\n",
    "        file.write(str(item) + '\\n')\n",
    "\n",
    "file.close()       \n",
    "        \n",
    "with open(\"true_clean.txt\", 'w') as file:\n",
    "    for item in true_data:\n",
    "        file.write(str(item) + '\\n')\n",
    "        \n",
    "file.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qD9asa6D5I5Y"
   },
   "source": [
    "Do tfidf analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating lables\n",
    "#A label of 1 is fake news, a label of 0 is real news\n",
    "\n",
    "labels = [1] * len(fake_data) + [0] * len(true_data)\n",
    "\n",
    "#splitting data into training and testing sets\n",
    "all_articles = fake_data + true_data\n",
    "training_articles, testing_articles, training_labels, testing_labels = train_test_split(all_articles, labels, random_state=42, test_size = 0.2)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(input='content', stop_words='english', decode_error = \"ignore\")\n",
    "tfidf_vector = tfidf_vectorizer.fit_transform(training_articles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classify With RFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4066  181]\n",
      " [ 132 4601]]\n",
      "true positives: 4601\n",
      "false negatives: 132\n",
      "true negatives: 4066\n",
      "false positives: 181\n"
     ]
    }
   ],
   "source": [
    "one_word_model = RandomForestClassifier(n_estimators = 50, max_depth = 10)\n",
    "one_word_model.fit(tfidf_vector, training_labels)\n",
    "tfidf_vector = tfidf_vectorizer.transform(testing_articles)\n",
    "\n",
    "predictions = one_word_model.predict(tfidf_vector)\n",
    "\n",
    "confused = confusion_matrix(testing_labels, predictions)\n",
    "print(confused)\n",
    "print(\"true positives: \" + str(confused[1][1]))\n",
    "print(\"false negatives: \" + str(confused[1][0]))\n",
    "print(\"true negatives: \"  +str(confused[0][0]))\n",
    "print(\"false positives: \" + str(confused[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(input='content', ngram_range=(2, 2), stop_words='english', decode_error = \"ignore\")\n",
    "count_vector = count_vectorizer.fit_transform(training_articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  feature\n",
      "israel trampl           1\n",
      "usa presid              1\n",
      "alexi tsipra            1\n",
      "turkey want             1\n",
      "avoid talk              1\n",
      "turkey erdogan          1\n",
      "erdogan speak           1\n",
      "athen talk              1\n",
      "said thursday           1\n",
      "solut island            1\n",
      "trampl law              1\n",
      "athen reuter            1\n",
      "usa jerusalem           1\n",
      "greek cypriot           1\n",
      "said turkey             1\n",
      "decis trampl            1\n",
      "unfortun decis          1\n",
      "recogn jerusalem        1\n",
      "erdogan say             1\n",
      "erdogan said            1\n",
      "cypru said              1\n",
      "said greek              1\n",
      "thursday usa            1\n",
      "presid donald           1\n",
      "trump unfortun          1\n"
     ]
    }
   ],
   "source": [
    "count_df= pd.DataFrame(count_vector[0].T.todense(), index=count_vectorizer.get_feature_names(), columns = [\"feature\"])\n",
    "first_25 = count_df.sort_values(by=[\"feature\"], ascending=[False]).head(25)\n",
    "\n",
    "print(first_25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2481 1766]\n",
      " [  27 4706]]\n",
      "true positives: 4706\n",
      "false negatives: 27\n",
      "true negatives: 2481\n",
      "false positives: 1766\n"
     ]
    }
   ],
   "source": [
    "two_word_model = RandomForestClassifier(n_estimators = 50, max_depth = 10)\n",
    "two_word_model.fit(count_vector, training_labels)\n",
    "count_vector = count_vectorizer.transform(testing_articles)\n",
    "\n",
    "predictions = two_word_model.predict(count_vector)\n",
    "\n",
    "confused = confusion_matrix(testing_labels, predictions)\n",
    "print(confused)\n",
    "print(\"true positives: \" + str(confused[1][1]))\n",
    "print(\"false negatives: \" + str(confused[1][0]))\n",
    "print(\"true negatives: \"  +str(confused[0][0]))\n",
    "print(\"false positives: \" + str(confused[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
